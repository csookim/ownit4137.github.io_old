---
title: "Do it! 딥러닝 입문 4"
date: 2020-08-12
categories: "MachineLearning" # 카테고리
excerpt: "DualLayer, MGD"
published : true # 공개

author_profile: false
header:
    teaser: "/assets/images/teaser/doitml.png"   # 작은 글일때의 이미지

toc: true #Table Of Contents 목차 보여줌
toc_label: " " # toc 이름 정의
toc_icon: " " #font Awesome아이콘으로 toc 아이콘 설정
toc_sticky: true # 스크롤 내릴때 같이 내려가는 목차
use_math: true # mathjax
---

# DualLayer

- class SingleLayer을 상속받음
- Random으로 가중치를 initialize
- BGD

## forpass, backprop

{% highlight py %}
class DualLayer(SingleLayer):

  def __init__(self, units = 10, learning_rate = 0.1, l1 = 0, l2 = 0):

  def forpass(self, x):
    z1 = np.dot(x, self.w1) + self.b1
    self.a1 = self.activation(z1)
    z2 = np.dot(self.a1, self.w2) + self.b2
    return z2

  def backprop(self, x, err):
    m = len(x)
    w2_grad = np.dot(self.a1.T, err) / m
    b2_grad = np.sum(err) / m
    h_err = np.dot(err, self.w2.T) * self.a1 * (1 - self.a1)
    w1_grad = np.dot(x.T, h_err) / m
    b1_grad = np.sum(h_err, axis = 0) / m

    return w1_grad, b1_grad, w2_grad, b2_grad

  def init_weights(self, n_features):
    self.w1 = np.random.normal(0, 1, (n_features, self.units))
    self.b1 = np.zeros(self.units)
    self.w2 = np.random.normal(0, 1, (self.units, 1))
    self.b2 = 0
{% endhighlight %}

- `h_err` 이 (m, 10)이고 b1이 (1, 10)이기 때문에 axis 0의 합을 구해 줘야 함
- [numpy.sum() axis](http://taewan.kim/post/numpy_sum_axis/)
- `w1` 과 `w2` 를 정규분포 평균 0, 표준편차 1 의 난수들로 초기화

## fit

{% highlight py %}
  def fit(self, x, y, x_val = None, y_val = None, epochs = 100):
    y = y.reshape(-1, 1)
    y_val = y_val.reshape(-1, 1)
    m = len(x)
    self.init_weights(x.shape[1])
    for i in range(epochs):
      a = self.training(x, y, m)
      a = np.clip(a, 1e-10, 1-1e-10)
      loss = np.sum(-(y * np.log(a) + (1 - y) * np.log(1 - a)))
      self.losses.append((loss + self.reg_loss()) / m)
      self.update_val_loss(x_val, y_val)

  def training(self, x, y, m):
    z = self.forpass(x)
    a = self.activation(z)
    err = -(y - a)
    w1_grad, b1_grad, w2_grad, b2_grad = self.backprop(x, err)
    w2_grad += (self.l1 * np.sign(self.w2) + self.l2 * self.w2) / m
    w1_grad += (self.l1 * np.sign(self.w1) + self.l2 * self.w1) / m

    self.w2 -= self.lr * w2_grad
    self.b2 -= self.lr * b2_grad
    self.w1 -= self.lr * w1_grad
    self.b1 -= self.lr * b1_grad
    return a

  def reg_loss(self):
    return self.l1 * (np.sum(np.abs(self.w1)) + np.sum(np.abs(self.w2))) + \
           self.l2 / 2 * (np.sum(self.w1**2) + np.sum(self.w2**2))
{% endhighlight %}

- forpass, backprop, GD 과정을 `training()` 으로 분리
- 각각의 w에 대한 규제 grad값 처리

## Mini-batch gradient descent

{% highlight py %}
class MinibatchNetwork(DualLayer):
  def __init__(self, units = 10, batch_size = 32, learning_rate = 0.1, l1 = 0, l2 = 0):
    super().__init__(units, learning_rate, l1, l2)
    self.batch_size = batch_size

  def fit(self, x, y, epochs = 100, x_val = None, y_val = None):
    y_val = y_val.reshape(-1, 1)
    self.init_weights(x.shape[1])
    for i in range(epochs):
      loss = 0
      for x_batch, y_batch in self.gen_batch(x, y):
        y_batch = y_batch.reshape(-1, 1)
        m = len(x_batch)
        a = self.training(x_batch, y_batch, m)
        a = np.clip(a, 1e-10, 1-1e-10)
        loss += np.sum(-(y_batch * np.log(a) + (1 - y_batch) * np.log(1 - a)))
      self.losses.append((loss + self.reg_loss()) / len(x))
      self.update_val_loss(x_val, y_val)

  def gen_batch(self, x, y):
    length = len(x)
    bins = length // self.batch_size
    if length % self.batch_size:
      bins += 1
    indexes = np.random.permutation(np.arange(len(x)))
    x = x[indexes]
    y = y[indexes]
    for i in range(bins):
      start = self.batch_size * i
      end = self.batch_size * (i + 1)
      yield x[start:end], y[start:end]
{% endhighlight %}

- super()을 이용해 부모 클래스의 내용을 상속
- epoch 마다 `batch_size` 만큼의 학습을 수행
- `np.random.permutation()` 을 통해 매 회 batch의 요소를 다르게 함
- `yield` : 함수 내에서 `yield` 를 사용하면 return을 실행한 후 함수를 종료하지 않고 유지, 다음 호출 시 다음 코드 실행

### batch_size에 따른 loss

{% highlight py %}
stochastic = MinibatchNetwork(l2 = 0.01, batch_size = 1)
minibatch32 = MinibatchNetwork(l2 = 0.01, batch_size = 32)
minibatch128 = MinibatchNetwork(l2 = 0.01, batch_size = 128)
batch = MinibatchNetwork(l2 = 0.01, batch_size = 350)

stochastic.fit(x_train_scaled, y_train, x_val = x_val_scaled, y_val = y_val, epochs = 100)
minibatch32.fit(x_train_scaled, y_train, x_val = x_val_scaled, y_val = y_val, epochs = 100)
minibatch128.fit(x_train_scaled, y_train, x_val = x_val_scaled, y_val = y_val, epochs = 100)
batch.fit(x_train_scaled, y_train, x_val = x_val_scaled, y_val = y_val, epochs = 100)

plt.plot(stochastic.losses)
plt.plot(minibatch32.losses)
plt.plot(minibatch128.losses)
plt.plot(batch.losses)

plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['stochastic', '32', '128', 'batch'])
plt.show()
{% endhighlight %}

![](/assets/posts/ml/bd91589a.png)

- `batch_size` 마다 계산량이 다르므로 정확도 차이가 있음
- `batch_size` 가 클수록 BGD와 가깝고, 작을수록 SGD와 가까운 개형
