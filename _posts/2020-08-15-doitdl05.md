---
title: "Do it! 딥러닝 입문 5"
date: 2020-08-15
categories: "MachineLearning" # 카테고리
excerpt: "MulticlassNetw"
published : true # 공개

author_profile: false
header:
    teaser: "/assets/images/teaser/doitml.png"   # 작은 글일때의 이미지

toc: true #Table Of Contents 목차 보여줌
toc_label: " " # toc 이름 정의
toc_icon: " " #font Awesome아이콘으로 toc 아이콘 설정
toc_sticky: true # 스크롤 내릴때 같이 내려가는 목차
use_math: true # mathjax
---


# 다중 분류

## function Softmax

- softmax함수를 사용하여 출력층의 출력 강도를 정규화시킴(출력값의 합을 1로 만듦)
- 각각의 출력값들이 적절한 확률값으로 변환됨
- 3-class 이상을 classification할 때 사용

## function Cross-Entropy Loss

\sum_{c=1}^c y_{c}log(a_{c})

- [about info theory](http://blog.naver.com/PostView.nhn?blogId=gyrbsdl18&logNo=221013188633)
- [Cross-Entropy func](https://89douner.tistory.com/28)
- 로지스틱 손실 함수는 Cross-Entropy 손실 함수를 이진 분류에 맞게 변형한 것

손실함수에서 $y_{c}$(= 예측해야할 클래스)가 1일 때, $a_{c}$(= 예측값) 가 1에 가까워야 함숫값이 작아지고 $y_{c}$가 0일 때, $a_{c}$ 가 0에 가까워야 함숫값이 작아짐
{: .notice}

# class MulticlassNetwork

{% highlight py %}
class MultiClassNetwork:

  def __init__(self, units = 10, batch_size = 32, lr = 0.1, l1 = 0, l2 = 0):
    ...

  def forpass(self, x):
    ...
    self.a1 = self.sigmoid(z1)
    ...

  def backprop(self, x, err):
    ...

  def sigmoid(self, z):
    a = 1 / (1 + np.exp(-z))
    return a

  def softmax(self, z):
    exp_z = np.exp(z)
    return exp_z / np.sum(exp_z , axis = 1).reshape(-1, 1)

  def init_weights(self, n_features, n_classes):
    ...
    self.w2 = np.random.normal(0, 1, (self.units, n_classes))
    self.b2 = np.zeros(n_classes)

  def fit(self, x, y, epochs = 100, x_val = None, y_val = None):
    self.init_weights(x.shape[1], y.shape[1])

    for i in range(epochs):
      loss = 0
      print('.', end = '') # no '\n'
      ...

  def gen_batch(self, x, y):
      ...

  def training(self, x, y):
    m = len(x)
    z = self.forpass(x)
    a = self.softmax(z)
    err = -(y - a)
    ...
    return a

  def predict(self, x):
    z = self.forpass(x)
    return np.argmax(z, axis = 1) # z(1, n_classes)

  def score(self, x, y):
    return np.mean(np.argmax(y, axis = 1) == self.predict(x))

  def reg_loss(self):
    ...

  def update_val_loss(self, x_val, y_val):
    ...
    a = self.softmax(z)
    ...
    val_loss = np.sum(-y_val * np.log(a))
    ...
{% endhighlight %}

- 출력층 activation함수에 softmax함수를 사용
- 손실 함수의 grad를 구할 때 사용되는 편미분 계산 $ \partial L/ \partial z  = -(y - a)$
- 출력층 activation함수, 손실 함숫값 계산 코드 수정
- `print(, end = '')` : 끝에 개행문자를 출력하지 않음
- `np.argmax()` : 가장 큰 값의 인덱스를 반환

{% highlight py %}

{% endhighlight %}


{% highlight py %}

{% endhighlight %}


{% highlight py %}

{% endhighlight %}


{% highlight py %}

{% endhighlight %}


{% highlight py %}

{% endhighlight %}


{% highlight py %}

{% endhighlight %}


{% highlight py %}

{% endhighlight %}


{% highlight py %}

{% endhighlight %}

{% highlight py %}

{% endhighlight %}


{% highlight py %}

{% endhighlight %}


{% highlight py %}

{% endhighlight %}
