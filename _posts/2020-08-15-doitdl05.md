---
title: "Do it! 딥러닝 입문 5"
date: 2020-08-15
categories: "MachineLearning" # 카테고리
excerpt: "MulticlassNetw"
published : true # 공개

author_profile: false
header:
    teaser: "/assets/images/teaser/doitml.png"   # 작은 글일때의 이미지

toc: true #Table Of Contents 목차 보여줌
toc_label: " " # toc 이름 정의
toc_icon: " " #font Awesome아이콘으로 toc 아이콘 설정
toc_sticky: true # 스크롤 내릴때 같이 내려가는 목차
use_math: true # mathjax
---


# 다중 분류

## function Softmax

- softmax함수를 사용하여 출력층의 출력 강도를 정규화시킴(출력값의 합을 1로 만듦)
- 각각의 출력값들이 적절한 확률값으로 변환됨
- 3-class 이상을 classification할 때 사용

## function Cross-Entropy Loss

$\sum_{c=1}^c y_{c}log(a_{c})$

- [about info theory](http://blog.naver.com/PostView.nhn?blogId=gyrbsdl18&logNo=221013188633)
- [Cross-Entropy func](https://89douner.tistory.com/28)
- 로지스틱 손실 함수는 Cross-Entropy 손실 함수를 이진 분류에 맞게 변형한 것

손실함수에서 $y_{c}$(= 예측해야할 클래스)가 1일 때, $a_{c}$(= 예측값) 가 1에 가까워야 함숫값이 작아지고 $y_{c}$가 0일 때, $a_{c}$ 가 0에 가까워야 함숫값이 작아짐
{: .notice}

# class MulticlassNetwork

{% highlight py %}
class MultiClassNetwork:

  def __init__(self, units = 10, batch_size = 32, lr = 0.1, l1 = 0, l2 = 0):
    ...

  def forpass(self, x):
    ...
    self.a1 = self.sigmoid(z1)
    ...

  def backprop(self, x, err):
    ...

  def sigmoid(self, z):
    a = 1 / (1 + np.exp(-z))
    return a

  def softmax(self, z):
    exp_z = np.exp(z)
    return exp_z / np.sum(exp_z , axis = 1).reshape(-1, 1)

  def init_weights(self, n_features, n_classes):
    ...
    self.w2 = np.random.normal(0, 1, (self.units, n_classes))
    self.b2 = np.zeros(n_classes)

  def fit(self, x, y, epochs = 100, x_val = None, y_val = None):
    self.init_weights(x.shape[1], y.shape[1])

    for i in range(epochs):
      loss = 0
      print('.', end = '') # no '\n'
      ...

  def gen_batch(self, x, y):
      ...

  def training(self, x, y):
    m = len(x)
    z = self.forpass(x)
    a = self.softmax(z)
    err = -(y - a)
    ...
    return a

  def predict(self, x):
    z = self.forpass(x)
    return np.argmax(z, axis = 1) # z(1, n_classes)

  def score(self, x, y):
    return np.mean(np.argmax(y, axis = 1) == self.predict(x))

  def reg_loss(self):
    ...

  def update_val_loss(self, x_val, y_val):
    ...
    a = self.softmax(z)
    ...
    val_loss = np.sum(-y_val * np.log(a))
    ...
{% endhighlight %}

- 출력층 activation함수에 softmax함수를 사용
- 손실 함수의 grad를 구할 때 사용되는 편미분 계산 $ \partial L/ \partial z  = -(y - a)$
- 출력층 activation함수, 손실 함숫값 계산 코드 수정
- `print(, end = '')` : 끝에 개행문자를 출력하지 않음
- `np.argmax()` : 가장 큰 값의 인덱스를 반환

## preprocessing

{% highlight py %}
(x_train_all, y_train_all), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()
x_train, x_val, y_train, y_val
        = train_test_split(x_train_all, y_train_all, stratify=y_train_all, test_size = 0.2)

x_train = x_train / 255
x_val = x_val / 255

x_train = x_train.reshape(-1, 784)
x_val = x_val.reshape(-1, 784)

y_train_encoded = tf.keras.utils.to_categorical(y_train)
y_val_encoded = tf.keras.utils.to_categorical(y_val)

fc = MultiClassNetwork(units = 100, batch_size = 256)
fc.fit(x_train, y_train_encoded, x_val = x_val, y_val = y_val_encoded, epochs = 40)
fc.score(x_val, y_val_encoded)
  # 0.8225
{% endhighlight %}

- `fashion_mnist` : 의류 데이터, 60000개의 샘플, 28*28 픽셀의 흑백 이미지
- 255로 나눠 scaling하고 MulticlassNetwork에 학습시키기 위해 feature을 1차원 배열로 reshape
- `to_categorical()` : one-hot encoding, 범주형 변수를 유의미한 숫자 벡터로 바꿔주는 것
- [one-hot encoding](https://minjejeon.github.io/learningstock/2017/06/05/easy-one-hot-encoding.html)

## 예측

{% highlight py %}
class_names = ['티셔츠', '바지', '스웨터', '드레스',
               '코트', '샌들', '셔츠', '스니커즈', '가방', '앵클부츠']

draw = test
test = test / 255
test = test.reshape(-1, 784)

for i in range(5):
  plt.imshow(draw[i], cmap = 'gray')
  plt.show()
  index = fc.predict(test)[i]
  print('predict : ', class_names[index])
  print('answer : ' , class_names[test_ans[i]])
{% endhighlight %}

- train set에서 무작위로 10개를 골라 predict해보고 결과를 비교

![](/assets/posts/ml/33d412ba.png)



# TensorFlow

- 구글에서 만든 딥러닝 라이브러리
- 시각화를 위한 TensorBoard를 제공
- [텐서플로 기본](https://excelsior-cjh.tistory.com/151?category=940399)

## install

{% highlight py %}
!pip install tensorflow_gpu==2.0.0

import tensorflow as tf
{% endhighlight %}


## class Sequential, Dense

{% highlight py %}
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense

model = Sequential()
model.add(Dense(100, activation = 'sigmoid', input_shape = (784, )))  # 은닉층
model.add(Dense(10, activation = 'softmax'))  # 출력층
model.compile(optimizer = 'sgd', loss = 'categorical_crossentropy', metrics = ['accuracy'])
{% endhighlight %}

- `Sequential` : 완전 연결 신경망 클래스
- `Dense` : 신경망에 포함된 각각의 연결층 클래스
- `compile()` : optimizer에 최적화 알고리즘을 지정, loss에 손실 함수를 지정
- `History` 객체에 loss와 accuracy를 기록 가능
