---
title: "[Concept] Classification 01"
date: 2020-11-15
categories: "MachineLearning" # 카테고리
excerpt: "DecisionTree, some Ensemble methods"
published : true # 공개

author_profile: false
header:
  teaser: "/_images/teaser/category/ml.png"


toc: true #Table Of Contents 목차 보여줌
toc_label: " " # toc 이름 정의
toc_icon: " " #font Awesome아이콘으로 toc 아이콘 설정
toc_sticky: true # 스크롤 내릴때 같이 내려가는 목차
use_math: true # mathjax
---


<span><span class="Python"><i class="fab fa-python"></i> Python</span><span class="PythonVer">3.6.9</span></span>&nbsp;&nbsp;
<span><span class="Sklearn"><i class="fas fa-chalkboard"></i> scikit-learn</span><span class="SklearnVer">0.22.2</span></span>

# 분류기 (Classifier)


## 결정 트리

- 쉽고 직관적, 데이터 사전 가공의 영향이 매우 적음
- 과적합이 발생하기 쉬움
- 스무고개 게임과 유사

### 정보 균일도

- 엔트로피 지수 : 주어진 데이터가 얼마나 혼잡한지(값이 섞여 있는지)를 의미, 수식
- 정보 이득(Information Gain) : 주어진 데이터가 얼마나 균일한지(종류가 비슷한지)를 의미, 1 - 엔트로피 지수로 나타냄
- 지니 계수(gini) : 경제학에서 불평등 지수를 나타낼 때 사용, 얼마나 혼잡한지를 의미

균일도(값 종류가 비슷함)↑ = 정보 이득↑ = 엔트로피↓ = 지니 계수↓
{: .notice}

### DecisionTreeClassifier

- sklearn에 존재
- 여러 가지 param을 조절해서 과적합을 방지

| param | default | description |
| :---- | :------ | :-----|
| `min_samples_split` | 2 | 노드 분할이 가능한 최소 데이터 수 |
| `min_samples_leaf` | 1 | 말단 노드가 되는 최소 데이터 수 |
| `max_features` | None(모든 feature) | 분할 시 고려할 최대 feature 개수 |
| `max_depth` | None(inf) | 최대 깊이 |
| `max_leaf_nodes` | None | 말단 노드의 최대 개수 |

- Graphviz 모듈을 이용해 결정 트리를 시각화

# 앙상블

- 여러 개의 Classifier을 결합


## Voting

- 서로 다른 분류기를 통해 학습하고, 투표를 통해 예측
- 하드 보팅 방식 : 각각의 예측값들 중 다수의 분류기가 예측한 값을 선정
- 소프트 보팅 방식 : 각 분류기의 레이블 값 예측 확률을 더하고 평균을 구해 평균이 가장 높은 예측값을 선정(분류기1 : [0.8, 0.2], 분류기2 : [0.4, 0.6] 이면 평균 : [**0.6**, 0.4])

### VotingClassifier

- 생성 인자로 `estimators` : 분류기들을 포함한 튜플, `voting` : 보팅 방식(hard, soft)를 받음

## Bagging

- 같은 분류기를 통해 학습하고, 투표를 통해 예측
- 각각 분류기들은 bootstrapping(bootstrap aggregating, 중첩을 허용하여 분리)된 데이터를 학습
- 분류기로 결정 트리를 사용한 것이 랜덤 포레스트

### RandomForestClassifier

- 하이퍼 파라미터가 많아 시간이 많이 소모됨, 병렬 처리는 가능

| param | default | description |
| :---- | :------ | :-----|
| `n_estimators` | 10 | 사용되는 결정 트리의 개수 |
| `max_features` | auto(sqrt) | 분할 시 고려할 최대 feature 개수 |

## Boosting

- 여러 개의 약한 학습기(weak learner)를 순차적으로 학습
- 이전 학습기에서 틀린 데이터에 가중치를 부여(틀린 데이터의 존재감이 커짐, 분류 기준을 세우는 데 영향을 미침)하여 오류를 개선
- AdaBoost(Adaptive Boosting) : Boosting한 학습기 여러개에 가중치를 매겨 결과를 냄
- GBM(Gradient BM) : AdaBoost와 유사, 가중치 업데이트 시 경사 하강법(GD)를 사용, 매우 느림(균형 트리 분할 방식)
- XGBoost(eXtra) : GBM기반, 과적합, 속도 등 문제 개선, 병렬 학습 가능, **캐글 고득점 알고리즘**
- LightGBM : 리프 중심 트리 분할 방식을 사용하여 학습 시간 개선, 10,000건 이하의 데이터를 사용할 때는 주의(과적합에 취약)

### GradientBoostingClassifier

| param | default | description |
| :---- | :------ | :-----|
| `loss` | deviance | 사용될 비용 함수 |
| `learning_rate` | 0.1 | 학습률 |
| `n_estimators` | 100 | 사용되는 결정 트리의 개수 |

### xgboost(python)

- python 내부의 xgboost 패키지 호출, 사이킷런과 호환 불가

| param | default | description |
| :---- | :------ | :-----|
| `eta` | 0.3 | 학습률 |

- **업데이트 필요**

### XGBClassifier(sklearn)

- fit(), predict() 사용 가능, 다른 sklearn여러 유틸리티 사용 가능

| param | default | description |
| :---- | :------ | :-----|
| `learning_rate` | 0.1 | 학습률 |

- **업데이트 필요**


### LGBMClassifier

| param | default | description |
| :---- | :------ | :-----|
| `learning_rate` | 0.1 | 학습률 |

- **업데이트 필요**


>  *권철민, 『파이썬 머신러닝 완벽 가이드』, 위키북스(2019)*
