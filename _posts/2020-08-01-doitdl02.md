---
title: "Do it! 딥러닝 입문 2"
date: 2020-07-30
categories: "MachineLearning" # 카테고리
excerpt: "로지스틱 회귀"
published : true # 공개

author_profile: false
header:
    teaser: "/assets/images/teaser/doitml.png"   # 작은 글일때의 이미지

toc: true #Table Of Contents 목차 보여줌
toc_label: " " # toc 이름 정의
toc_icon: " " #font Awesome아이콘으로 toc 아이콘 설정
toc_sticky: true # 스크롤 내릴때 같이 내려가는 목차
use_math: true # mathjax
---

# Code

{% highlight py %}
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import numpy as np

cancer = load_breast_cancer()
x = cancer.data
y = cancer.target
{% endhighlight %}

## Cancer 자료 분석

{% highlight py %}
plt.boxplot(cancer.data)
cancer.feature_names[[3, 13, 23]] #indexing
  # result : array(['mean area', 'area error', 'worst area'], dtype='<U23')

np.unique(cancer.target, return_counts=True)
  # (array([0, 1]), array([212, 357]))
{% endhighlight %}


![](/assets/posts/ml/c4e59845.png)

- `boxplot` 함수를 이용해 data의 분포를 확인
- `feature_names` 함수를 이용해 분포가 큰 특성의 이름을 확인
- `unique` 함수를 이용해 들어있는 값의 종류와 횟수(return_counts = True)를 반환

## Dataset 분리

{% highlight py %}
x_train, x_test, y_train, y_test = train_test_split(x, y, stratify = y, test_size = 0.2, random_state = 42)
np.unique(y_train, return_counts = True)
  # (array([0, 1]), array([170, 285]))
{% endhighlight %}

- 데이터 세트를 training set와 test set으로 나눠줘야 함
- 데이터 세트의 클래스(종류) 비율이 유지되어야 함
- unique 함수를 이용해 비율이 유지됨을 확인(212:357 ≈ 170:285)

### train_test_split()

- `stratify = y` : 데이터 세트 클래스 비율을 유지함(y 에 target이 들어감)
- `test_size = 0.2` : test set의 비율을 의미, default는 0.25
- `random_state = 42` : 데이터 세트를 무작위로 섞을 때의 시드값, 값이 고정되면 set이 매번 동일하게 나옴


## class Neuron

{% highlight py %}
class LogisticNeuron:

  def __init__(self):
    self.w = None
    self.b = None

  # x(30, ) 와 w(30, ) 를 각각 곱하고 b를 더해 리턴
  def forpass(self, x):
    z = np.sum(x * self.w) + self.b
    return z

  # gradient 를 리턴
  def backprop(self, x, err):
    w_grad = x * err
    b_grad = 1 * err
    return w_grad, b_grad

  # logistic function
  def activation(self, z):
    a = 1 / (1 + np.exp(-z))
    return a

  def fit(self, x, y, epoch=100):
    self.w = np.ones(x.shape[1])
    self.b = 0
    for i in range(epoch):
      for x_i, y_i in zip(x, y):
        z = self.forpass(x_i)
        a = self.activation(z)
        err = -(y_i - a)
        w_grad, b_grad = self.backprop(x_i, err)
        self.w -= w_grad
        self.b -= b_grad

  """
  x_test(114, 30) 의 각 행을 forpass하여
  얻은 a(30, )값이 0.5보다 작은지(30, )를 리턴
  """
  def predict(self, x):
    z = [self.forpass(x_i) for x_i in x]
    a = self.activation(np.array(z))
    return a > 0.5
{% endhighlight %}


- 모든 y에 대해 `y_hat` 과 `y_i` 가 얼마나 차이가 있는지를 나타내는 것이 cost function($L$), 정의역은 w와 b
- cost 값을 최소로 만들기 위해 적절한 w와 b 탐색이 필요
- $L$을 w와 b에 대해 미분한 값을( `w_grad` , `b_grad` ) 업데이트 해줌으로써 gradient descent를 수행


{% highlight py %}
neuron = LogisticNeuron()
neuron.fit(x_train, y_train)
np.mean(neuron.predict(x_test) == y_test) # 0.825..
{% endhighlight %}

![](/assets/posts/ml/64c66e74.png)
