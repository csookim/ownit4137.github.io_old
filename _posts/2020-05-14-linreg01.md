---
title: "Linear Regression"
date: 2020-05-14
categories: "MachineLearning" # 카테고리
excerpt: "Learning Algorithm: Linear Regression"
published : true # 공개

author_profile: false
header:
    teaser: "/assets/images/teaser/mllec.jpg"   # 작은 글일때의 이미지

toc: true #Table Of Contents 목차 보여줌
toc_label: " " # toc 이름 정의
toc_icon: " " #font Awesome아이콘으로 toc 아이콘 설정
toc_sticky: true # 스크롤 내릴때 같이 내려가는 목차
use_math: true
---

> Machine Learning by Andrew Ng WEEK 1, 2

# 선형 회귀(LP)

## 학습 모델

**학습 데이터**

|$i$ | $x$ | $y$ |
|:--:|:---:|:---:|
| 1  | 230 | 3.5 |
| 2  | 142 | 5.2 |
|... | ... | ... |
| $m$  |$x^{(i)}$|$y^{(i)}$|

- $m$ : 학습 예제의 수
- $x^{(i)}$ : input변수, (input변수가 여러 가지일 때는 ${x^{(i)}}_0, {x^{(i)}}_1 ...$ 등으로 표시)
- $y^{(i)}$ : output변수, 출력값
- $h(x)$ : 가설(hypothesis) 함수, $x$에서 $y$로 가는 함수
- $h_{\theta}(x) = \theta_{0}+\theta_{1}x + ...$  으로 나타낼 수 있음
- ${\theta}_{n}$ : Parameter, 가설 함수의 개형을 결정 $\rightarrow$ **비용 함수의 함숫값** 을 결정

## 비용 함수(Cost Function)

### 정의
- 데이터와 가설 함수 사이의 **차이 제곱의 평균$\times\frac{1}{2}$** 을 의미
- 미분 계산의 편의를 위해 2로 나눠줌
- This function is otherwise called the "Squared error function", or "Mean squared error".


$J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}\left(h_\theta({x}^{(i)})-{y}^{(i)} \right)^2$
{: .notice--info}

### Contour Plot

- $\theta$변수가 두 가지 일 때 그래프의 모양은 아래와 같음
![cp01](/assets/posts/ml/cp01.jpg)
- $\theta_n$에 따른(비용 함수 그래프에서 x 표시된 점) $h_\theta(x)$ 그래프의 개형
![cp02](/assets/posts/ml/cp02.jpg)
- 비용이 최소일 때 $h_\theta(x)$ 그래프의 개형
![cp03](/assets/posts/ml/cp03.jpg)


비용이 최소일 때의 $h_\theta(x)$는 Linear Regressing된 함수로써 예측에 관여
{: .notice--info}

## Gradient Descent

- 비용 함수의 최솟값을 구하는 알고리즘
- 수렴할 때 까지 $\theta_n$에 대해 편미분한 값을 빼서 대입
- 어떤 점에서의 $f_{x}, f_{y}$값이 0이 될 때(이차원) 임계점이 되므로 함수 $J$를 **모든 $\theta$값으로 편미분 한 값이 0** 이 되게 만들어야 함
- 최소점일 때에는 편미분 값이 0이 되어 값이 바뀌지 않음

![gd01](https://user-images.githubusercontent.com/57739683/81960021-49050200-964b-11ea-887e-c8fc804588b3.jpg)

- The optimization problem we have posed here for linear regression **has only one global, and no other local, optima**; thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum.



# 다변량 선형 회귀(LP with multiple variables)

## 학습 모델

**학습 데이터**

|$i$ | $x_1$ | $x_2$ | $x_3$ | $y$ |
|:--:|:-----:|:-----:|:-----:|:---:|
| 1  | 230   |   1   |  -1.7 | 3.5 |
| 2  | 142   |   3   |  -2   | 5.2 |
|... |  ...  |  ...  |  ...  | ... |
| $m$|$x^{(i)}_1$|$x^{(i)}_2$|$x^{(i)}_3$|$y^{(i)}$|

- 각각의 ${x^{(i)}}_j$들을 **feature** 라고 함
- $x^{(i)}$ 의 변수가 여러 가지이므로 벡터로 나타냄
- 벡터는 열의 개수가 1 인 행렬

$$
x^{(2)}=
\begin{bmatrix}
142\\
3\\
-2
\end{bmatrix}
$$

- 다변량일 때에는 가설 함수를 행렬로 표기
- 편의상 ${x^{(i)}_0}=1$ 로 사용
- $x = \begin{bmatrix}x_0\\\\ x_1\\\\ {\vdots}\end{bmatrix} \theta = \begin{bmatrix}\theta_0\\\\ \theta_1\\\\ \vdots\end{bmatrix}$ 일 때

$h_{\theta}(x^{(i)})=\theta^T{x^{(i)}}=\theta_{0}+\theta_{1}{x^{(i)}}\_1 + \theta_{2}{x^{(i)}}\_2 + ...$
{: .notice--info}


## Gradient Descent

![dg02](https://user-images.githubusercontent.com/57739683/82079130-4a066400-971d-11ea-9e11-1bbb285e9a55.jpg)


- $\alpha$는 학습률(learning rate)
- 미분하여 ${x^{(i)}}_j$ 가 곱해짐
- 동시 대입 필요


### Learning Rate

- 학습률 $\alpha$는 Gradient Descent에 영향을 미침
- 0.001 ~ 1 사이의 적절한 값을 찾는 것이 중요


If $\alpha$ is small enough,  you find that the value of $J(\theta)$ decreases quickly then levels off.
{: .notice--info}


## Feature Scaling

- 만약 각각의 feature들의 범위의 크기가 다르다면, contour plot이 뾰족한 타원형을 띄게 되고 난해한 경로를 따라 gradient descent를 하게 됨
- mean normalization을 이용해서 feature들의 값을 약 -1에서 1 사이로 맞춰줌(정규화)


$x_{i}  :=  \frac{x_{i}-\mu_{i}}{s_{i}}$

$\mu_{i}$ : average of feature $x$

$s_{i}$ : StDev or range of feature $x$


Feature Scaling을 사용하여 **더 효율적인** gradient descent를 수행할 수 있음
{: .notice--info}


## 다항 회귀(Polynomial Regression)

- 여러 가지 feature을 조합해서 새로운 feature를 나타낼 때, 가설 함수 그래프의 개형이 곡선형을 띔

![pr01](https://user-images.githubusercontent.com/57739683/82076559-fb56cb00-9718-11ea-95af-e06d11f575e9.jpg)

- 각각의 feature식을 새로운 문자로 치환하여 선형식을 만들 수 있음
- scaling할때는 범위에 맞게 조정 필요

$h_{\theta}(x)=\theta_{0}+\theta_{1}x^5 + \theta_{2}\sqrt{x}$일 때, $x^5=x_{1}, \sqrt{x}=x_{2}$로 치환
{: .notice--info}


## Normal Equation

- 행렬의 연산을 이용하여 $\theta$를 효율적으로 계산
- 행렬의 inverse 연산을 수행하므로 $n<10^5$일때 사용
- column 1 에는 ${x^{(i)}}_0=1$로 채워짐

$X=\begin{bmatrix}(x^{(1)})^T\\\\ (x^{(2)})^T\\\\ {\vdots}\\\\ (x^{(m)})^T\end{bmatrix}   y=\begin{bmatrix}y^{(1)}\\\\ y^{(2)}\\\\ {\vdots}\\\\y^{(m)}\end{bmatrix}$일 때


$\theta = (X^{T}X)^{-1}X^{T}y$
{: .notice--info}


| Normal Equation | Gradient Descent |
| :-------------- | :--------------- |
| $O(n^3)$        | $O(n^2)$         |
| 반복문, $\alpha$계산 x | 반복문, $\alpha$계산 필요|
