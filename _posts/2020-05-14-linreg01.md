---
title: "Linear Regression 1"
date: 2020-05-14
categories: "MachineLearning" # 카테고리
excerpt: "Learning Algorithm: Linear Regression"
published : true # 공개

author_profile: false
header:
    teaser: "/assets/images/teaser/mllec.jpg"   # 작은 글일때의 이미지

toc: true #Table Of Contents 목차 보여줌
toc_label: " " # toc 이름 정의
toc_icon: " " #font Awesome아이콘으로 toc 아이콘 설정
toc_sticky: true # 스크롤 내릴때 같이 내려가는 목차
use_math: true
---

> Machine Learning by Andrew Ng WEEK 1

# 선형 회귀(LP)

## 학습 모델

**학습 데이터**

|$i$ | $x$ | $y$ |
|:--:|:---:|:---:|
| 1  | 230 | 3.5 |
| 2  | 142 | 5.2 |
|... | ... | ... |
| $m$  |$x^{(i)}$|$y^{(i)}$|

$m$ : 학습 예제의 수

$x^{(i)}$ : input변수, (input변수가 여러 가지일 때는 ${x^{(i)}}_0, {x^{(i)}}_1 ...$ 등으로 표시)

$y^{(i)}$ : output변수, 출력값


$h(x)$ : 가설(hypothesis) 함수, $x$에서 $y$로 가는 함수

${h}_{\theta}(x) = {\theta}_{0}+{\theta}_{1}x + ...$  으로 나타낼 수 있음

${\theta}_{n}$ : Parameter, 가설 함수의 개형을 결정 $\rightarrow$ **비용 함수의 함숫값** 을 결정

## 비용 함수(Cost Function)

### 정의
- 데이터와 가설 함수 사이의 **차이 제곱의 평균$\times\frac{1}{2}$** 을 의미
- 미분 계산의 편의를 위해 2로 나눠줌
- This function is otherwise called the "Squared error function", or "Mean squared error".


$J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}\left(h_\theta({x}^{(i)})-{y}^{(i)} \right)^2$
{: .notice--info}

### Contour Plot

- $\theta$변수가 두 가지 일 때 그래프의 모양은 아래와 같음
![cp01](https://user-images.githubusercontent.com/57739683/81956070-d9404880-9645-11ea-86b7-9711b03273ac.jpg)

- $\theta_n$에 따른(비용 함수 그래프에서 x 표시된 점) $h_\theta(x)$ 그래프의 개형
![cp02](https://user-images.githubusercontent.com/57739683/81956148-ed844580-9645-11ea-972e-cabcca74ea7f.jpg)
- 비용이 최소일 때 $h_\theta(x)$ 그래프의 개형
![cp03](https://user-images.githubusercontent.com/57739683/81956152-eeb57280-9645-11ea-9e67-3b450b3f47cb.jpg)


비용이 최소일 때의 $h_\theta(x)$는 Linear Regressing된 함수로써 예측에 관여
{: .notice--info}

### Gradient Descent

- 비용 함수의 최솟값을 구하는 알고리즘
- 수렴할 때 까지 $\theta_n$에 대해 편미분한 값을 계산하여 대입
- 최소점일 때에는 편미분 값이 0이 되어 값이 바뀌지 않음

![gd01](https://user-images.githubusercontent.com/57739683/81960021-49050200-964b-11ea-887e-c8fc804588b3.jpg)

- The optimization problem we have posed here for linear regression **has only one global, and no other local, optima**; thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum.

# 다변량 선형 회귀(LP with multiple variables)

**학습 데이터**

|$i$ | $x_1$ | $x_2$ | $x_3$ | $y$ |
|:--:|:-----:|:-----:|:-----:|:---:|
| 1  | 230   |   1   |  -1.7 | 3.5 |
| 2  | 142   |   3   |  -2   | 5.2 |
|... |  ...  |  ...  |  ...  | ... |
| $m$|$x^{(i)}_1$|$x^{(i)}_2$|$x^{(i)}_3$|$y^{(i)}$|

- $x^{(i)}$ 의 변수가 여러 가지이므로 벡터로 나타냄
- 벡터는 열의 개수가 1 인 행렬

$$
x^{(2)}=
\begin{bmatrix}
142\\
3\\
-2
\end{bmatrix}
$$

- 다변량일 때에는 가설 함수를 행렬로 표기
- 편의상 ${x^{(i)}_0}=1$ 로 사용

$
x^{(i)}=
\begin{bmatrix}
x^{(i)}_0\\
x^{(i)}_1\\
x^{(i)}_2\\
...
\end{bmatrix}
$&nbsp;&nbsp;&nbsp; $
\theta=
\begin{bmatrix}
\theta_0\\
\theta_1\\
\theta_2\\
...
\end{bmatrix}
$일 때,

${h}_{\theta}(x^{(i)}) = \theta^T{x^{(i)}}
= {\theta}_{0}+{\theta}_{1}{x^{(i)}_1} + {\theta}_{2}{x^{(i)}_2} + ...$
