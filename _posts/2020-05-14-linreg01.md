---
title: "Linear Regression 1"
date: 2020-05-14
categories: "MachineLearning" # 카테고리
excerpt: "Learning Algorithm: Linear Regression"
published : true # 공개

author_profile: false
header:
    teaser: "/assets/images/teaser/mllec.jpg"   # 작은 글일때의 이미지

toc: true #Table Of Contents 목차 보여줌
toc_label: " " # toc 이름 정의
toc_icon: " " #font Awesome아이콘으로 toc 아이콘 설정
toc_sticky: true # 스크롤 내릴때 같이 내려가는 목차
use_math: true
---

> Machine Learning by Andrew Ng WEEK 1

# 학습 모델

**학습 데이터**
|$i$ | $x$ | $y$ |
|:--:|:---:|:---:|
| 1  | 230 | 3.5 |
| 2  | 142 | 5.2 |
|... | ... | ... |
| $m$  |$x^{(i)}$|$y^{(i)}$|

$m$ : 학습 예제의 수

$x^{(i)}$ : input변수, (input변수가 여러 가지일 때는 ${x^{(i)}}_0, {x^{(i)}}_1 ...$ 등으로 표시)

$y^{(i)}$ : output변수, 출력값


$h(x)$ : 가설(hypothesis) 함수, $x$에서 $y$로 가는 함수

${h}_{\theta}(x) = {\theta}_{0}+{\theta}_{1}x + ...$  으로 나타낼 수 있음

${\theta}_{n}$ : Parameter, 가설 함수의 개형을 결정 $\rightarrow$ **비용 함수의 함숫값** 을 결정

# 비용 함수(Cost Function)

## 정의
- 데이터와 가설 함수 사이의 **차이 제곱의 평균$\times\frac{1}{2}$** 을 의미
- 미분 계산의 편의를 위해 2로 나눠줌
- This function is otherwise called the "Squared error function", or "Mean squared error".


$J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^{m}\left(h_\theta({x}^{(i)})-{y}^{(i)} \right)^2$
{: .notice--info}

## Contour Plot

- $\theta$변수가 두 가지 일 때 그래프의 모양은 아래와 같음
![cp01](https://user-images.githubusercontent.com/57739683/81956070-d9404880-9645-11ea-86b7-9711b03273ac.jpg)

- $\theta_n$에 따른(비용 함수 그래프에서 x 표시된 점) $h_\theta(x)$ 그래프의 개형
![cp02](https://user-images.githubusercontent.com/57739683/81956148-ed844580-9645-11ea-972e-cabcca74ea7f.jpg)
- 비용이 최소일 때 $h_\theta(x)$ 그래프의 개형 (데이터들을 가로지름)
![cp03](https://user-images.githubusercontent.com/57739683/81956152-eeb57280-9645-11ea-9e67-3b450b3f47cb.jpg)


비용이 최소일 때의 $h_\theta(x)$는 Linear Regressing된 함수로써 예측에 관여
{: .notice--info}
