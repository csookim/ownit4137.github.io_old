---
title: "Neural Network 2"
date: 2020-05-28
categories: "MachineLearning" # 카테고리
excerpt: "Backpropagation & Implementation"
published : true # 공개

author_profile: false
header:
    teaser: "/assets/images/teaser/mllec.jpg"   # 작은 글일때의 이미지

toc: true #Table Of Contents 목차 보여줌
toc_label: " " # toc 이름 정의
toc_icon: " " #font Awesome아이콘으로 toc 아이콘 설정
toc_sticky: true # 스크롤 내릴때 같이 내려가는 목차
use_math: true
---


> Machine Learning by Andrew Ng WEEK 5

# Neural Network

![](/assets/posts/ml/edbda49d.png)

## Cost Function

![](/assets/posts/ml/cc0de6dd.png)

- 모든 input $x^{(i)}$에 대해 Logistic Regression cost함수를(함숫값이 작을수록 정확한 예측)적용한 값에다 모든 $\Theta$ 요소를 더함(Overfitting 방지)
- the $i$ in the triple sum does not refer to training example $i$

## Backpropagation(역전파)

- Neural Network의 Cost function을 최소화(예측을 더 정확히) 하기 위한 알고리즘
- 한 개의 input을 **Forward prop한 후 진행** 됨
- output layer에서 최종 오차를 계산한 후, 오차에 가중치를 적용해 다시 분배
- 경사하강법을 이용($J$를 모든 가중치$\Theta$로 편미분한 값을 줄여 나가는 과정)
- $\frac{\partial J}{\partial w}$에 연쇄법칙을 사용

![](/assets/posts/ml/6473e420.png)

![](/assets/posts/ml/ee13261a.png)

그림에서 각각의 가중치($\Theta$)가 ${a_{k}}^{(L-1)}$이 ${a_{0}}^{(L)}, {a_{1}}^{(L)}$의 값에 미치는 영향을 의미하므로 ${a_{k}}^{(L-1)}$에서 오차가 발생했을 때, 그 값도 $\Theta$만큼의 영향을 미칠 것이다. 따라서 ${a_{0}}^{(L)}, {a_{1}}^{(L)}$의 오차를 구하여 각각의 $\Theta$만큼 곱한 후 더해주면($g'$ 계산 필요) 역으로 ${a_{k}}^{(L-1)}$의 오차를 구할 수 있다. 그 후 각각의 $\Theta$에 오차를 계산해주어 가중치를 최적화한다.
{: .notice--info}


[Backpropagation Calculus by 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)


## Unrolling

- 실제 수행될 코드에서는 $\Theta$행렬의 크기가 크고 개수도 많을 것이므로 이를 column이 1인 벡터로 **Unroll** 하여 함수의 인자 넣기나 연산을 편리하게 함
- 함수가 unroll된 벡터를 받고 reshape하여 연산한 후 결과를 unroll하여 리턴


## Gradient Checking

- Backpropagation은 굉장히 민감하므로 디버깅 과정이 필요함

![](/assets/posts/ml/ca01f92d.png)

- 모든 $\Theta_{j}$의 근처에서 기울기 근사를 하여 $J$의 편미분 값과 비교
- Gradient Checking 코드는 디버깅 과정에서 포함하고, 실행 과정에서는 제외(매우 느림)

## Random Initialization

- 모든 가중치를 동일한 값으로 초기화한다면 input layer을 제외한 layer의 값이 동일해짐
- 랜덤함수를 사용해 가중치에 대한 symmetry breaking을 수행
