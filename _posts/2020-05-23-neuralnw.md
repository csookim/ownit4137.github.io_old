---
title: "Neural Network"
date: 2020-05-23
categories: "MachineLearning" # 카테고리
excerpt: "Learning Algorithm: Neural Network"
published : true # 공개

author_profile: false
header:
    teaser: "/assets/images/teaser/mllec.jpg"   # 작은 글일때의 이미지

toc: true #Table Of Contents 목차 보여줌
toc_label: " " # toc 이름 정의
toc_icon: " " #font Awesome아이콘으로 toc 아이콘 설정
toc_sticky: true # 스크롤 내릴때 같이 내려가는 목차
use_math: true
---

> Machine Learning by Andrew Ng WEEK 4

# Neural Network

![](/assets/posts/ml/963805bf.png)

- 그림과 같이 학습 과정에서 복잡한 비선형 가설함수를 필요로 할 때 다변수 고차항을 추가하는 데에는 한계가 있음
- 시냅스의 결합으로 네트워크를 형성한 인공 뉴런을 학습시키는 Neural Network를 사용

## 학습 모델

![](/assets/posts/ml/d5a84894.png)
![](/assets/posts/ml/be8350a3.png)

- Layer : 같은 계층에 있는 뉴런(=퍼셉트론, unit)들의 집합, 위치에 따라 input, hidden, output layer로 나뉨
- ${a_{i}}^{(j)}$ : "activation" of unit $i$ in layer $j$
- $\Theta^{(j)}$ : **matrix of weights** controlling function mapping from layer $j$ to $j+1$
- $x_{0}, {a_{0}}^{(j)}$ : "bias unit" It is always equal to 1
- $g(z)$ : "activation function" Sigmoid function in this lecture
- $s_{j}$ : the number of units in layer $j$


If network has $s_{j}$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $\Theta^{(j)}$ will be of dimension $s_{j+1} * s_{j}+1$. (rows from the number of units in layer $j+1$, columns from the number of **units and bias node** in layer $j$)
{: .notice--info}
