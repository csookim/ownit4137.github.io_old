---
title: "Neural Network"
date: 2020-05-23
categories: "MachineLearning" # 카테고리
excerpt: "Learning Algorithm: Neural Network"
published : true # 공개

author_profile: false
header:
    teaser: "/assets/images/teaser/mllec.jpg"   # 작은 글일때의 이미지

toc: true #Table Of Contents 목차 보여줌
toc_label: " " # toc 이름 정의
toc_icon: " " #font Awesome아이콘으로 toc 아이콘 설정
toc_sticky: true # 스크롤 내릴때 같이 내려가는 목차
use_math: true
---


> Machine Learning by Andrew Ng WEEK 4

# Neural Network

![](/assets/posts/ml/963805bf.png)

- 그림과 같이 학습 과정에서 복잡한 비선형 가설함수를 필요로 할 때 다변수 고차항을 추가하는 데에는 한계가 있음
- 시냅스의 결합으로 네트워크를 형성한 인공 뉴런을 학습시키는 Neural Network를 사용


## 학습 모델

![](/assets/posts/ml/d5a84894.png)
![](/assets/posts/ml/be8350a3.png)

- Layer : 같은 계층에 있는 뉴런(=unit)들의 집합, 위치에 따라 input, hidden, output layer로 나뉨
- ${a_{i}}^{(j)}$ : "activation" of unit $i$ in layer $j$
- $\Theta^{(j)}$ : **matrix of weights** controlling function mapping from layer $j$ to $j+1$
- $x_{0}, {a_{0}}^{(j)}$ : "bias unit" It is always equal to 1
- $g(z)$ : "activation function" Sigmoid function in this lecture
- $s_{j}$ : the number of units in layer $j$


If network has $s_{j}$ units in layer $j$ and $s_{j+1}$ units in layer $j+1$, then $\Theta^{(j)}$ will be of dimension $s_{j+1} \times s_{j}+1$. (rows from the number of units in layer $j+1$, columns from the number of **units and bias node** in layer $j$)
{: .notice--info}


## Classification Example

- by using sigmoid function
- 각 숫자들은 $\Theta^{(1)}$을 의미

### AND

<img alt="1c230e16.png" src="/assets/posts/ml/1c230e16.png" width="360px">

- 선형 가설 함수로 classify 가능

### OR

<img alt="d50313ca.png" src="/assets/posts/ml/d50313ca.png">

- 선형 가설 함수로 classify 가능

### NOR

<img alt="8b552b72.png" src="/assets/posts/ml/8b552b72.png" width="350px">

- 선형 가설 함수로 classify 가능

### NXOR

<img alt="639f07c4.png" src="/assets/posts/ml/639f07c4.png">

- **비선형 가설 함수** 가 필요하지만 Neural Network을 사용하여 classify 가능
- And there we have the XNOR operator using a hidden layer with two nodes.


## Multiclass Classification

![](/assets/posts/ml/8cc408a6.png)

- output layer을 row가 class개인 행렬로 만들어 각각이 1일때로 분류
- 가설 함수의 결과를 행렬로 도출

![](/assets/posts/ml/12cb5e84.png)


## Vectorized Implementation

### One class

$
a^{(2)} = g(\Theta^{(1)} \cdot x)
\\a^{(3)} = g(\Theta^{(2)} \cdot a^{(2)})
\\...
\\h_{\Theta}(x) = a^{(j)}
$

### Multiclass

$
a^{(2)} = g(\Theta^{(1)} \cdot x)
\\a^{(3)} = g(\Theta^{(2)} \cdot a^{(2)})
\\...
\\{(h_{\Theta}(x))}\_{i} = {a_{i}}^{(j)}
$


## 원리

A라는 물체가 있는 어떤 사진(흑백) P가 있다고 할 때 각각의 픽셀 값($x$)에 가중치($\Theta$)를 매겨 단일 layer 모델 L을 제작했다고 가정하자. 어떤 사진 X가 정확히 P인지 아닌지 구별할 때는 L을 사용하여 충분히 예측할 수 있다. 하지만 X에 물체 A가 있는가를 판단할 때는 L을 사용할 수 없다. 단순한 픽셀들의 가중치만을 담고 있기 때문에 A의 위치나 크기가 달라졌을 때 A인지 분간할 수 없다는 것이 그 이유이다.

이를 해결하기 위해 여러 layer로 구성된 Neural Network를 사용한다. $x$가 각각의 픽셀이라고 할 때, 곡선을 그리는 좌표들에 가중치를 부여한 $a^{(2)}$라는 층들을 만들 수 있고, 그것을 기반으로 하여 원을 그리는 좌표들에 가중치를 부여한 $a^{(3)}$이라는 층들을 만들 수 있다.

layer가 거듭될수록 각 ${a_{i}}^{(j)}$은 더 복잡한 조건을 가지게 된다.

만약 $h_{\Theta}(X) = a^{(j)} = $ "사진 X에 있는 물체 A는 사과이다" 일때,

- ${a_{1}}^{(j-1)}$은 "물체가 원 모양이다",
- ${a_{2}}^{(j-1)}$은 "물체에 꼭지가 존재한다",
- ${a_{3}}^{(j-1)}$은 "물체의 표면에 줄무늬가 있다"

등이 될 수 있을 것이다. (${a_{3}}^{(j-1)}$에 매우 적은 가중치가 부여될 것이라고 예상할 수 있다.) 이렇게 점점 더 복잡한 조건을 판단하는 뉴런들을 이용해 layer을 쌓아 가는 것이 Neural Network이다.
