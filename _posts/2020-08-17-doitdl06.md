---
title: "Do it! 딥러닝 입문 6"
date: 2020-08-17
categories: "MachineLearning" # 카테고리
excerpt: "Convolutional Neural Network"
published : true # 공개

author_profile: false
header:
    teaser: "/assets/images/teaser/doitml.png"   # 작은 글일때의 이미지

toc: true #Table Of Contents 목차 보여줌
toc_label: " " # toc 이름 정의
toc_icon: " " #font Awesome아이콘으로 toc 아이콘 설정
toc_sticky: true # 스크롤 내릴때 같이 내려가는 목차
use_math: true # mathjax
---

# CNN(Convolutional Neural Network)

- 시각적 이미지를 학습하는데 사용되는 신경망
- 뇌의 뉴런들이 시야에 있는 특정한 시각 자극에 반응하여 물체를 인식한다는 점에서 착안함
- 합성곱 연산 대신 교차 상관 연산이 이루어지지만 관습적으로 합성곱 신경망이라고 부름
- [합성곱 신경망 설명](https://www.youtube.com/watch?v=ggBQj1NXUEg)
- [합성곱 신경망](https://excelsior-cjh.tistory.com/180)

# class ConvolutionalNetwork

![](/assets/posts/ml/325cf353.png)


## forpass, init_weights

{% highlight py %}
class ConvolutionalNetwork:

  def __init__(self, n_kernels = 10, units = 10, batch_size = 32, learning_rate = 0.1):
    self.n_kernels = n_kernels
    self.kernel_size = 3
    self.optimizer = None
    self.conv_w = None
    self.conv_b = None
    self.units = units
    self.batch_size = batch_size
    self.w1 = None
    self.b1 = None
    self.w2 = None
    self.b2 = None
    self.a1 = None
    self.losses = []
    self.val_losses = []
    self.lr = learning_rate

  def forpass(self, x):
    c_out = tf.nn.conv2d(x, self.conv_w, strides = 1, padding = 'SAME') + self.conv_b
    r_out = tf.nn.relu(c_out)
    p_out = tf.nn.max_pool2d(r_out, ksize = 2, strides = 2, padding = 'VALID')
    f_out = tf.reshape(p_out, [x.shape[0], -1])
            # (batch_size, 14 * 14 * 10)
    z1 = tf.matmul(f_out, self.w1) + self.b1
    a1 = tf.nn.relu(z1)
    z2 = tf.matmul(a1, self.w2) + self.b2
    return z2

  def init_weights(self, input_shape, n_classes):
    g = tf.initializers.glorot_uniform()
    self.conv_w = tf.Variable(g((3, 3, 1, self.n_kernels)))
    self.conv_b = tf.Variable(np.zeros(self.n_kernels), dtype = float)
    n_features = 14 * 14 * self.n_kernels
    self.w1 = tf.Variable(g((n_features, self.units)))
    self.b1 = tf.Variable(np.zeros(self.units), dtype = float)
    self.w2 = tf.Variable(g((self.units, n_classes)))
    self.b2 = tf.Variable(np.zeros(n_classes), dtype = float)  
{% endhighlight %}

- 합성곱층 : $3 \times 3$ 커널 10개, 절편 10개
- 완전 연결층 : DualLayer, 각각 층을 이루는 가중치, 절편
- `forpass()` : 합성곱 연산(SAME) → activate(ReLU) → 풀링(max, VALID) → SGD
- 내장 함수를 사용하기 위해 `softmax` 함수를 이용한 연산을 **수행하지 않음**
- 연산에는 `Tensor` 객체가 사용되므로 `tf.matmul()` 을 사용
- `tf.reshape()` 는 `np.reshape()` 와 **사용 형식이 다름**
- 자동 미분을 사용하기 위해 변수에 `tf.Variable` 적용

### Glorot Initialization

- [Glorot init](https://reniew.github.io/13/)
- [Glorot init](https://lv99.tistory.com/23)
- 가중치의 입력층과 출력층의 뉴런 개수를 이용하여 범위를 정하고, 그 사이에서 균등하게 난수를 발생시킴


## fit

{% highlight py %}
...

  def fit(self, x, y, epochs = 100, x_val = None, y_val = None):
    self.init_weights(x.shape, y.shape[1])
    self.optimizer = tf.optimizers.SGD(learning_rate = self.lr)
    for i in range(epochs):
      print('에포크', i, end = ' ')
      batch_losses = []
      for x_batch, y_batch in self.gen_batch(x, y):
        print('.', end = '')
        self.training(x_batch, y_batch)
        batch_losses.append(self.get_loss(x_batch, y_batch))
      print()
      self.losses.append(np.mean(batch_losses))
      self.val_losses.append(self.get_loss(x_val, y_val))

  def get_loss(self, x, y):
    z = self.forpass(x)
    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, z))
    return loss.numpy()

  def gen_batch(self, x, y):
    bins = len(x) // self.batch_size
    indexes = np.random.permutation(np.arange(len(x)))
    x = x[indexes]
    y = y[indexes]
    for i in range(bins):
      start = self.batch_size * i
      end = self.batch_size * (i + 1)
      yield x[start:end], y[start:end]

...
{% endhighlight %}


- 경사 하강법을 사용하기 위해 `optimizer` 객체를 SGD로 설정
- 매 epoch마다 `self.losses` 에 training 샘플당 평균 `loss` 와 val 샘플당 평균 `loss` 를 기록
- `tf.reduce_mean()` 와 `np.mean()` 은 axis에 따른 행렬의 평균을 리턴
- `tf.nn.s_c_e_w_l(y, z)` : z에다 softmax처리 → cross entropy loss 계산
- [what is logits?](https://stackoverflow.com/questions/34240703/what-is-logits-softmax-and-softmax-cross-entropy-with-logits)

## training

{% highlight py %}
...

  def training(self, x, y):
    m = len(x)
    with tf.GradientTape() as tape:
      z = self.forpass(x)
      loss = tf.nn.softmax_cross_entropy_with_logits(y, z)
      loss = tf.reduce_mean(loss)
    weights_list = [self.conv_w, self.conv_b, self.w1, self.b1, self.w2, self.b2]
    grads = tape.gradient(loss, weights_list)
    self.optimizer.apply_gradients(zip(grads, weights_list))

  def predict(self, x):
    z = self.forpass(x)
    return np.argmax(x.numpy(), axis = 1)

  def score(self, x, y):
    return np.mean(self.predict(x) == np.argmax(y, axis = 1))
    
...
{% endhighlight %}

### tf.GradientTape

- [about GradientTape](https://www.tensorflow.org/tutorials/customization/autodiff?hl=ko)
- Tensorflow는 자동 미분 기능을 지원함
- with 블럭으로 감싸진 코드에서 일어나는 모든 내용을 tf.GradientTape(= tape) 객체에 기록함
- 내부에 있는 `tf.Variable` 객체를 자동으로 추적
- 샘플당 평균 loss를 구하고 각 weights_list에 대해 미분
- 설정해두었던 SGD로 grad을 업데이트

# class Conv2D, MaxPooling2D

## Code

- `Conv2D` : keras에 내장되어 있는 합성곱층 클래스
- `MaxPooling2D` : keras에 내장되어 있는 최대 풀링층 클래스
- `Flatten` : 풀링된 커널을 일렬로 펼침

{% highlight py %}
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# 전체 신경망
conv1 = tf.keras.Sequential()
# 합성곱층
conv1.add(Conv2D(10, (3 , 3),
          activation = 'relu', padding = 'same', input_shape = (28, 28, 1)))
# 풀링층
conv1.add(MaxPooling2D((2, 2)))
# Flatten층
conv1.add(Flatten())
# Dropout층
conv1.add(Dropout(0.5))
# 완전 연결층
conv1.add(Dense(100, activation = 'relu'))
conv1.add(Dense(10, activation = 'softmax'))


conv1.compile(optimizer = 'adam',
                  loss = 'categorical_crossentropy', metrics = ['accuracy'])
history = conv1.fit(x_train, y_train_encoded, epochs = 20,
                  validation_data = (x_val, y_val_encoded))
{% endhighlight %}

### Adam(Adaptive Moment Estimation)

- 최적화 알고리즘으로 **Adam** 을 사용
- [about Adam](https://light-tree.tistory.com/141)

### Dropout

- 모델을 훈련시킬 때, 과적합을 방지하기 위해서 사용하는 방법
- 가중치 layer 안에 있는 임의의 뉴런들을 0으로 만들어 특정 뉴런에 과도하게 의존되는 것을 방지
- Dropout을 수행하면 실전 출력 값이 일정 비율로 커짐
- [Dropout](https://pythonkim.tistory.com/42)
