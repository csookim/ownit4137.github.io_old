---
title: "Logistic Regression 1"
date: 2020-05-16
categories: "MachineLearning" # 카테고리
excerpt: "Learning Algorithm: Logistic Regression"
published : true # 공개

author_profile: false
header:
    teaser: "/assets/images/teaser/mllec.jpg"   # 작은 글일때의 이미지

toc: true #Table Of Contents 목차 보여줌
toc_label: " " # toc 이름 정의
toc_icon: " " #font Awesome아이콘으로 toc 아이콘 설정
toc_sticky: true # 스크롤 내릴때 같이 내려가는 목차
use_math: true
---

> Machine Learning by Andrew Ng WEEK 3

# 로지스틱 회귀(LP)

## 데이터 모델

**학습 데이터**

|$i$ | $x$ | $y$ |
|:--:|:---:|:---:|
| 1  | 230 |  1  |
| 2  | 142 |  0  |
| 3  | 347 |  0  |
|... | ... | ... |
| $m$  |$x^{(i)}$|$y^{(i)}$|

- output변수인 $y^{(i)}$가 0과 1의 값만을 가짐
- 결과를 **이산적** 으로 나타낼 수 있는 모델에서 사용(특정 메일의 스팸 여부, 종양의 악성 여부 등)
- $\theta^{T}x = \theta_{0}+\theta_{1}x + ...$ (from previous post)
- $g(x) = \frac{1}{1+ e^{-x} }$ : Sigmoid(Logistic)함수, 결과값이 확률로 나오므로 확률 해석에 사용
- $h_{\theta}(x) = g(\theta^{T}x)$
- $h_{\theta}(x)$에 sigmoid function을 사용하여 함숫값이 (0, 1)에 위치하게 하고 그 함숫값은 **output이 1일 확률** 을 나타냄

$h_{\theta}(x) = P(y=1\|x;\theta) = 1-P(y=0\|x;\theta)$
{: .notice--info}


### Decision Boundary

- output의 값을 결정하는 경계
- $\theta^{T}x \geq 0 \Rightarrow y=1$
- $\theta^{T}x < 0 \Rightarrow y=0$


## 비용 함수(Cost Function)

- We cannot use the same cost function(Mean squared error) that we use for linear regression because the Logistic Function will **cause the output to be wavy, causing many local optima.**
- convex를 만들기 위해 새로운 cost함수를 정의한다.


$J( \theta )= -\frac{1}{m}  \sum_{i=1}^m [y^{(i)}\log(h_{\theta}(x^{(i)}))+(1-y^{(i)})\log(1-h_{\theta}(x^{(i)}))] $
{: .notice--info}

A vectorized implementation is:

$h=g(X \theta ) , J( \theta )= -\frac{1}{m}\big(y^{T}\log(h)+(1-y)^{T}\log(1-h)\big)$
{: .notice--info}


## Gradient Descent

![image](https://user-images.githubusercontent.com/57739683/82474958-b959ca80-9b06-11ea-90ca-c4fa4dc0ff4c.png)

함수의 최적화에는 Gradient Descent 보다 복잡하지만 더 빠른 "Conjugate gradient", "BFGS", and "L-BFGS" 등의 알고리즘이 존재한다.
{: .notice--info}

# 다중 클래스 분류

- One-vs-all
- 두 가지 이상의 분류 카테고리를 $y$에다 이산적인 값으로 할당한다.
- $y=i$일때, **$i$번째 카테고리 데이터 vs 나머지** 로 학습시켜 $h_{\theta}(x^{(i)})$를 구한다.
- $h_{\theta}(x^{(i)}) = P(y=i\|x;\theta)$ , $y$값이 $i$일 확률

![image](https://user-images.githubusercontent.com/57739683/82475897-2cb00c00-9b08-11ea-9690-b962e2cb99cf.png)


예측할 variable $x$를 모든 $h_{\theta}(x^{(i)})$ 에 대입해 보고 값(확률)이 가장 클때의 $i$ 를 $y$ 값으로 예측
{: .notice--info}
